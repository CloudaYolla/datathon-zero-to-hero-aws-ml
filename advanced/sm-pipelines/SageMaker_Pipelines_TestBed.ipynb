{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- Create a processing job that does nothing but create a print() output.\n",
    "- Call it from the notebook.\n",
    "- Pass in a parameter to the processing job from the notebook and use the results in the print() stateement.\n",
    "(Now you can launch the processing job)\n",
    "\n",
    "- Create a small notebook (checkout the examples in article 5) that loads a model and evaluates a model, as uses some test data (you can see getting the testdata in our Reddit notebook)\n",
    "(Now you can load and use a model directly)\n",
    "- And with the metrics I donâ€™t know where we are. What we can use from experiments. If nothing we go for what we do today in the Reddit noteb\n",
    "\n",
    "## References\n",
    "* [1] https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker_processing/scikit_learn_data_processing_and_model_evaluation/scikit_learn_data_processing_and_model_evaluation.ipynb\n",
    "* [2] https://sagemaker-examples.readthedocs.io/en/latest/sagemaker_processing/scikit_learn_data_processing_and_model_evaluation/scikit_learn_data_processing_and_model_evaluation.html\n",
    "* [3] This one is for BYOC for running external scripts (preferred): https://docs.aws.amazon.com/sagemaker/latest/dg/processing-container-run-scripts.html\n",
    "* [4] This one is for BYOC including code: https://docs.aws.amazon.com/sagemaker/latest/dg/build-your-own-processing-container.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SK LEARN Hello World Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.20.0\",\n",
    "    role=role,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing processing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile processing.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelBinarizer, KBinsDiscretizer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "\n",
    "def print_shape():\n",
    "    print('Data shape: {}, {} positive examples, {} negative examples')\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train-test-split-ratio', type=float, default=0.3)\n",
    "    parser.add_argument('--dummy-argument', type=str, default=\"hello somebody\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    print('Received arguments {}'.format(args))\n",
    "\n",
    "    input_data_path = os.path.join('/opt/ml/processing/input', 'lorem.csv')\n",
    "    print('Reading input data from {}'.format(input_data_path))\n",
    "    \n",
    "    \n",
    "    print('Train data shape after preprocessing: {}')    \n",
    "    train_features_output_path = os.path.join('/opt/ml/processing/train', 'lorem.csv')\n",
    "    \n",
    "    print('Saving training features to {}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "sklearn_processor.run(\n",
    "    code=\"processing.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(source=\"s3://akirmak-r/text_classification.yaml\", destination=\"/opt/ml/processing/input\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train_data\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"test_data\", source=\"/opt/ml/processing/test\"),\n",
    "    ],\n",
    "    arguments=[\"--train-test-split-ratio\", \"0.2\", \"--dummy-argument\", \"hello NLP article team\"],\n",
    ")\n",
    "\n",
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PYTORCH Evaluation HELLO WORLD SAMPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Container \n",
    "\n",
    "IMPORTANT: Run this from outside SM Studio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## IMPORTANT: Run this from outside SM Studio)\n",
    "\n",
    "# import boto3\n",
    "\n",
    "# account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "# region = boto3.Session().region_name\n",
    "# ecr_repository = 'akirmak-smprocessing'\n",
    "# tag = ':latest'\n",
    "# processing_repository_uri = '{}.dkr.ecr.{}.amazonaws.com/{}'.format(account_id, region, ecr_repository + tag)\n",
    "\n",
    "# # Create ECR repository and push docker image\n",
    "# !docker build -t $ecr_repository docker\n",
    "# !aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin {account_id}.dkr.ecr.{region}.amazonaws.com\n",
    "# !aws ecr create-repository --repository-name $ecr_repository\n",
    "# !docker tag {ecr_repository + tag} $processing_repository_uri\n",
    "# !docker push $processing_repository_uri\n",
    "\n",
    "# processing_repository_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "FROM 763104351884.dkr.ecr.eu-west-1.amazonaws.com/pytorch-inference:1.8.1-cpu-py36-ubuntu18.04\n",
    "#FROM python:3.7-slim-buster\n",
    "\n",
    "#RUN pip3 install pytorch\n",
    "\n",
    "RUN pip3 install sagemaker sagemaker-experiments transformers nltk tensorboard sklearn jsonlines boto3\n",
    "# Add a Python script and configure Docker to run it\n",
    "ENTRYPOINT [\"python3\", \"inference.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing build_container.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile build_container.sh\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "# Run from Terminal, e.g. from Cloud9\n",
    "docker build -t 'akirmak-smprocessing' docker \n",
    "aws ecr get-login-password --region eu-west-1 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.eu-west-1.amazonaws.com\n",
    "aws ecr get-login-password --region eu-west-1 | docker login --username AWS --password-stdin 123456789012.dkr.ecr.eu-west-1.amazonaws.com\n",
    "aws ecr create-repository --repository-name 'akirmak-smprocessing'\n",
    "docker tag akirmak-smprocessing:latest '123456789012.dkr.ecr.eu-west-1.amazonaws.com/akirmak-smprocessing:latest'\n",
    "docker push '123456789012.dkr.ecr.eu-west-1.amazonaws.com/akirmak-smprocessing:latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "# region = boto3.Session().region_name\n",
    "# ecr_repository = 'akirmak-smprocessing'\n",
    "# tag = ':latest'\n",
    "# processing_repository_uri = '{}.dkr.ecr.{}.amazonaws.com/{}'.format(account_id, region, ecr_repository + tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting build_container-AUto.sh\n"
     ]
    }
   ],
   "source": [
    "# %%writefile build_container-AUto.sh\n",
    "# #!/usr/bin/env bash\n",
    "\n",
    "# # Run from Terminal, e.g. from Cloud9\n",
    "# # Create ECR repository and push docker image\n",
    "# docker build -t {ecr_repository} docker\n",
    "# aws ecr get-login-password --region eu-west-1 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.eu-west-1.amazonaws.com\n",
    "# aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin {account_id}.dkr.ecr.{region}.amazonaws.com\n",
    "# aws ecr create-repository --repository-name $ecr_repository\n",
    "# docker tag {ecr_repository + tag} $processing_repository_uri\n",
    "# docker push $processing_repository_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelBinarizer, KBinsDiscretizer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "\n",
    "def print_shape():\n",
    "    print('Data shape: {}, {} positive examples, {} negative examples')\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train-test-split-ratio', type=float, default=0.3)\n",
    "    parser.add_argument('--dummy-argument', type=str, default=\"hello somebody\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    print('THIS IS **********INFERENCE FROM PYTORCH****************************** CONTAINER')\n",
    "    print('Received arguments {}'.format(args))\n",
    "\n",
    "    input_data_path = os.path.join('/opt/ml/processing/input', 'lorem.csv')\n",
    "    print('Reading input data from {}'.format(input_data_path))\n",
    "    \n",
    "    \n",
    "    print('Train data shape after preprocessing: {}')    \n",
    "    train_features_output_path = os.path.join('/opt/ml/processing/train', 'lorem.csv')\n",
    "    \n",
    "    print('Saving training features to {}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "\n",
    "script_processor = ScriptProcessor(command=['python3'],\n",
    "                image_uri='123456789012.dkr.ecr.us-east-1.amazonaws.com/akirmak-smprocessing:latest',\n",
    "                role=role,\n",
    "                instance_count=1,\n",
    "                instance_type='ml.m5.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script_processor.run(code='inference.py',\n",
    "#                      inputs=[ProcessingInput(\n",
    "#                         source='s3://path/to/my/input-data.csv',\n",
    "#                         destination='/opt/ml/processing/input')],\n",
    "#                      outputs=[ProcessingOutput(source='/opt/ml/processing/output/train'),\n",
    "#                                ProcessingOutput(source='/opt/ml/processing/output/validation'),\n",
    "#                                ProcessingOutput(source='/opt/ml/processing/output/test')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_processor.run(\n",
    "    code=\"inference.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(source=\"s3://akirmak-r/text_classification.yaml\", destination=\"/opt/ml/processing/input\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train_data\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"test_data\", source=\"/opt/ml/processing/test\"),\n",
    "    ],\n",
    "    arguments=[\"--train-test-split-ratio\", \"0.2\", \"--dummy-argument\", \"hello NLP article team\"],\n",
    ")\n",
    "\n",
    "inference_job_description = script_processor.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "!aws s3 ls {model_url}\n",
    "from nlc.jobutil import get_role\n",
    "role = get_role()\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data=model_url,\n",
    "                             role=role,\n",
    "                             py_version='py3',\n",
    "                             framework_version='1.8',\n",
    "                             source_dir='.',\n",
    "                             entry_point='inference.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-pipelines/tabular/abalone_build_train_deploy/sagemaker-pipelines-preprocess-train-evaluate-batch-transform.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_uri = 's3://prj-ml/banking.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "\n",
    "\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\", default_value=\"ml.m5.xlarge\"\n",
    ")\n",
    "training_instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=\"ml.m5.xlarge\")\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\"\n",
    ")\n",
    "input_data = ParameterString(\n",
    "    name=\"InputData\",\n",
    "    default_value=input_data_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"hba-SMPipelinesTestBed-Process\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n",
    "    ],\n",
    "    code=\"processing.py\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script_eval = 'inference.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\", output_name=\"evaluation\", path=\"evaluation.json\"\n",
    ")\n",
    "\n",
    "step_eval = ProcessingStep(\n",
    "    name=\"hba-evaluation-smprocessing\",\n",
    "    processor=script_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_process.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "    ],\n",
    "    code=\"inference.py\",\n",
    "    property_files=[evaluation_report]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "\n",
    "pipeline_name = f\"hbaSMPipelineTestBed\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        processing_instance_count,\n",
    "        training_instance_type,\n",
    "        model_approval_status,\n",
    "        input_data,\n",
    "    ],\n",
    "    steps=[step_process, step_eval],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Version': '2020-12-01',\n",
       " 'Metadata': {},\n",
       " 'Parameters': [{'Name': 'ProcessingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.m5.xlarge'},\n",
       "  {'Name': 'ProcessingInstanceCount', 'Type': 'Integer', 'DefaultValue': 1},\n",
       "  {'Name': 'TrainingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.m5.xlarge'},\n",
       "  {'Name': 'ModelApprovalStatus',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'PendingManualApproval'},\n",
       "  {'Name': 'InputData',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 's3://prj-ml/banking.csv'}],\n",
       " 'PipelineExperimentConfig': {'ExperimentName': {'Get': 'Execution.PipelineName'},\n",
       "  'TrialName': {'Get': 'Execution.PipelineExecutionId'}},\n",
       " 'Steps': [{'Name': 'hba-SMPipelinesTestBed-Process',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.m5.xlarge',\n",
       "      'InstanceCount': 1,\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.20.0-cpu-py3',\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/processing.py']},\n",
       "    'RoleArn': 'arn:aws:iam::1234567890:role/service-role/AmazonSageMaker-ExecutionRole-20180314T175147',\n",
       "    'ProcessingInputs': [{'InputName': 'input-1',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': 'Parameters.InputData'},\n",
       "       'LocalPath': '/opt/ml/processing/input',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-1234567890/sagemaker-scikit-learn-2021-08-29-14-33-07-802/input/code/processing.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'train',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-1234567890/sagemaker-scikit-learn-2021-08-29-14-33-07-802/output/train',\n",
       "        'LocalPath': '/opt/ml/processing/train',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'test',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-1234567890/sagemaker-scikit-learn-2021-08-29-14-33-07-802/output/test',\n",
       "        'LocalPath': '/opt/ml/processing/test',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}}},\n",
       "  {'Name': 'hba-evaluation-smprocessing',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.m5.xlarge',\n",
       "      'InstanceCount': 1,\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '1234567890.dkr.ecr.us-east-1.amazonaws.com/akirmak-smprocessing:latest',\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/inference.py']},\n",
       "    'RoleArn': 'arn:aws:iam::1234567890:role/service-role/AmazonSageMaker-ExecutionRole-20180314T175147',\n",
       "    'ProcessingInputs': [{'InputName': 'input-1',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': \"Steps.hba-SMPipelinesTestBed-Process.ProcessingOutputConfig.Outputs['test'].S3Output.S3Uri\"},\n",
       "       'LocalPath': '/opt/ml/processing/test',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-1234567890/akirmak-smprocessing-2021-08-29-14-33-07-909/input/code/inference.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'evaluation',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-1234567890/akirmak-smprocessing-2021-08-29-14-33-07-909/output/evaluation',\n",
       "        'LocalPath': '/opt/ml/processing/evaluation',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}},\n",
       "   'PropertyFiles': [{'PropertyFileName': 'EvaluationReport',\n",
       "     'OutputName': 'evaluation',\n",
       "     'FilePath': 'evaluation.json'}]}]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:1234567890:pipeline/hbasmpipelinetestbed',\n",
       " 'ResponseMetadata': {'RequestId': '49402554-bb10-4192-86af-19da3defef45',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '49402554-bb10-4192-86af-19da3defef45',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '88',\n",
       "   'date': 'Sun, 29 Aug 2021 14:33:11 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:1234567890:pipeline/hbasmpipelinetestbed',\n",
       " 'PipelineExecutionArn': 'arn:aws:sagemaker:us-east-1:1234567890:pipeline/hbasmpipelinetestbed/execution/j5alfiumea9y',\n",
       " 'PipelineExecutionDisplayName': 'execution-1630247594493',\n",
       " 'PipelineExecutionStatus': 'Executing',\n",
       " 'PipelineExperimentConfig': {'ExperimentName': 'hbasmpipelinetestbed',\n",
       "  'TrialName': 'j5alfiumea9y'},\n",
       " 'CreationTime': datetime.datetime(2021, 8, 29, 14, 33, 14, 431000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2021, 8, 29, 14, 33, 14, 431000, tzinfo=tzlocal()),\n",
       " 'CreatedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:1234567890:user-profile/d-1bf2r1y78nt5/akirmak-amazon-com-17e',\n",
       "  'UserProfileName': 'akirmak-amazon-com-17e',\n",
       "  'DomainId': 'd-1bf2r1y78nt5'},\n",
       " 'LastModifiedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:1234567890:user-profile/d-1bf2r1y78nt5/akirmak-amazon-com-17e',\n",
       "  'UserProfileName': 'akirmak-amazon-com-17e',\n",
       "  'DomainId': 'd-1bf2r1y78nt5'},\n",
       " 'ResponseMetadata': {'RequestId': '069fb867-9dcf-480e-a334-4dedc4fd7f0a',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '069fb867-9dcf-480e-a334-4dedc4fd7f0a',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '863',\n",
       "   'date': 'Sun, 29 Aug 2021 14:33:58 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_steps()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "\n",
    "viz = LineageTableVisualizer(sagemaker.session.Session())\n",
    "for execution_step in reversed(execution.list_steps()):\n",
    "    print(execution_step)\n",
    "    display(viz.show(pipeline_execution_step=execution_step))\n",
    "    time.sleep(5)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
